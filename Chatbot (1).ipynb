{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"m8Q8xDo3iB33"},"outputs":[],"source":[" #Install required libraries\n","!pip install openai==0.28\n","!pip install gradio==3.50\n","!pip install tensorflow==2.14"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d19c40b3"},"outputs":[],"source":["#Load installed library\n","import os\n","import openai\n","import gradio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GZYIsuVPDIw9"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xoo7sT9hB8fh"},"outputs":[],"source":["!pip list |grep gradio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-FIsglcCE4g5"},"outputs":[],"source":["!pip list |grep openai"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JOyQ5w9kN2X4"},"outputs":[],"source":["!pip list | grep tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1eaf5f2a"},"outputs":[],"source":["#Load chatgpt api key to access chatgpt\n","openai.api_key  = \"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\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pexHZEjkNyxd"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0cbd89c6"},"outputs":[],"source":["openai.api_key"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uj0sUAj3iM21"},"outputs":[],"source":["#Mount google grive to access contents\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"52krRtszA5TZ"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5-2fGT0djEh-"},"outputs":[],"source":["#Load saved lstm model\n","import joblib as jb\n","dep_detec = jb.load('/content/drive/MyDrive/mental_health/first_model.jb')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DXvq0EKqjTkO"},"outputs":[],"source":["#Change the current working directory\n","import os\n","os.chdir('/content/drive/MyDrive/mental_health')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"afc8ab39"},"outputs":[],"source":["import pickle as pk #load pickle library to get tokenizer file\n","import nltk #load nltk library for nlp task\n","nltk.download(['punkt','stopwords']) #download the diffrent stopwords available on nltk\n","from keras.preprocessing.text import Tokenizer #import tokenizer\n","from keras.preprocessing.sequence import pad_sequences #import pad_sequence to padd tweets/text\n","\n","#load the tokenizer file used during training and save it to a variable\n","with open('tokenizer.pickle', 'rb') as handle:\n","    tokenizer = pk.load(handle)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WSl6JIIYjlIN"},"outputs":[],"source":["import re\n","import string\n","import numpy as np\n","\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import TweetTokenizer\n","\n","\n","def process_tweet(tweet):\n","    \"\"\"Process tweet function.\n","    Input:\n","        tweet: a string containing a tweet\n","    Output:\n","        tweets_clean: a list of words containing the processed tweet\n","\n","    \"\"\"\n","    stemmer = PorterStemmer()\n","    stopwords_english = stopwords.words('english')\n","    # remove stock market tickers like $GE\n","    tweet = re.sub(r'\\$\\w*', '', tweet)\n","    # remove old style retweet text \"RT\"\n","    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n","    # remove hyperlinks\n","    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n","    # remove hashtags\n","    # only removing the hash # sign from the word\n","    tweet = re.sub(r'#', '', tweet)\n","    # tokenize tweets\n","    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n","                               reduce_len=True)\n","    tweet_tokens = tokenizer.tokenize(tweet)\n","\n","    tweets_clean = []\n","    for word in tweet_tokens:\n","        if (word not in stopwords_english and  # remove stopwords\n","                word not in string.punctuation):  # remove punctuation\n","            # tweets_clean.append(word)\n","            stem_word = stemmer.stem(word)  # stemming word\n","            tweets_clean.append(stem_word)\n","\n","    return tweets_clean\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"be16bc71"},"outputs":[],"source":["from nltk.corpus import stopwords"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"83388cde"},"outputs":[],"source":["stop_words = set(stopwords.words('english'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ac1536b"},"outputs":[],"source":["def process_multiple_tweets(tweets):\n","\n","    MAX_SEQUENCE_LENGTH = 2495 #pad each tweet to 2495 tokens\n","\n","    processed_tweets = [process_tweet(tweet) for tweet in tweets] #process the each tweet/text in the function\n","\n","    tweets_non_stopwords = [[word for word in tweet_words if not word in stop_words] #check for stopwords and remove them\n","              for tweet_words in processed_tweets]\n","\n","    #check for more stopwords and remove them\n","    collection_words = ['im', 'de', 'like', 'one']\n","    tweets_non_stopwords2 = [[w for w in word if not w in collection_words]\n","                 for word in tweets_non_stopwords]\n","    #Tokenize the processed tweets/texts\n","    text_to_seq = tokenizer.texts_to_sequences(tweets_non_stopwords2)\n","    pad_token_tweets = pad_sequences(text_to_seq,maxlen=MAX_SEQUENCE_LENGTH) #pad each tweets to have a length of 2495 which was the maximum length of tweets used during training\n","    return pad_token_tweets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"845583fc"},"outputs":[],"source":["def single_tweet_process(tweet):\n","    MAX_SEQUENCE_LENGTH = 2495\n","\n","    processed_tweet = process_tweet(tweet)\n","\n","    tweets_non_stopwords = [word for word in processed_tweet]\n","\n","    collection_words = ['im', 'de', 'like', 'one']\n","    tweets_non_stopwords2 = [[w for w in tweets_non_stopwords if not w in collection_words]]\n","    text_to_seq = tokenizer.texts_to_sequences(tweets_non_stopwords2)\n","\n","    pad_token_tweets = pad_sequences(text_to_seq,maxlen=MAX_SEQUENCE_LENGTH)\n","    return pad_token_tweets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"96b0483a"},"outputs":[],"source":["#Function to predict tweets/tweet\n","def predict_tweet(tweets):\n","    if len(tweets) == 1:\n","      #if the user just provides one tweet pass it to the single tweet function\n","        only_tweet = single_tweet_process(tweets[0])\n","        #predict the tweet using the model\n","        pred = dep_detec.predict(only_tweet)\n","        #if the prediction is greater than 0.5 the person is depressive\n","        if pred[0][0] \u003e 0.5:\n","            print(pred[0][0])\n","            print('Based On your Social media appearance you are depressive')\n","        else:\n","          #if the prediction is less than 0.5 the person is not depressive\n","            print('Based On your Social media appearance you are not depressive')\n","    elif len(tweets) \u003e 1:\n","      #if the user just provides one tweet pass it to the single tweet function\n","        all_tweets = process_multiple_tweets(tweets)\n","        pred = dep_detec.predict(all_tweets)\n","        if pred.mean() \u003e 0.5:\n","            return('Based on your social media appearance you sound depressive')\n","        else:\n","            return ('Based on your social media appearance you do notsound depressive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5cgEDLxBErOL"},"outputs":[],"source":["!pip list|grep openai"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":782},"id":"c0a00b7d"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/gradio/blocks.py:599: UserWarning: Cannot load compact. Caught Exception: The space compact does not exist\n","  warnings.warn(f\"Cannot load {theme}. Caught Exception: {str(e)}\")\n"]},{"name":"stdout","output_type":"stream","text":["Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n","Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n","\n","To create a public link, set `share=True` in `launch()`.\n"]},{"data":{"application/javascript":["(async (port, path, width, height, cache, element) =\u003e {\n","                        if (!google.colab.kernel.accessAllowed \u0026\u0026 !cache) {\n","                            return;\n","                        }\n","                        element.appendChild(document.createTextNode(''));\n","                        const url = await google.colab.kernel.proxyPort(port, {cache});\n","\n","                        const external_link = document.createElement('div');\n","                        external_link.innerHTML = `\n","                            \u003cdiv style=\"font-family: monospace; margin-bottom: 0.5rem\"\u003e\n","                                Running on \u003ca href=${new URL(path, url).toString()} target=\"_blank\"\u003e\n","                                    https://localhost:${port}${path}\n","                                \u003c/a\u003e\n","                            \u003c/div\u003e\n","                        `;\n","                        element.appendChild(external_link);\n","\n","                        const iframe = document.createElement('iframe');\n","                        iframe.src = new URL(path, url).toString();\n","                        iframe.height = height;\n","                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n","                        iframe.width = width;\n","                        iframe.style.border = 0;\n","                        element.appendChild(iframe);\n","                    })(7860, \"/\", \"100%\", 500, false, window.element)"],"text/plain":["\u003cIPython.core.display.Javascript object\u003e"]},"metadata":{},"output_type":"display_data"}],"source":["import openai\n","import gradio as gr\n","recent_post = [] #list to store the users inputted tweet/text\n","#this is to set the context of the chatgpt language model\n","messages = [ {'role':\"system\", \"content\": f\"\"\"\n","Follow the steps strictly\n","\n","\n","STEP 1: Introduce yourself as Social media mental health specialist and your name is Lizzy.\n","\n","Step 2: Tell the user in a polite manner to produce 2-10 of their most recent tweets or textual post on their various social media platform\n","\n","Step 3: Put all the post in the python list {recent_post}\n","\n","Step 4: Execute this function {predict_tweet(recent_post)} and output the function value in a descriptive and nice way.\n","\n","\"\"\"}]\n","\n","def chatbot(input):\n","    #input is the users text\n","    if input:\n","        #Append the user's text to the context set for the chatgpt language model\n","        messages.append({\"role\": \"user\", \"content\": input})\n","        #Send the whole message list to the chatgpt ai using the ChatCompletion function\n","        chat = openai.ChatCompletion.create(\n","            model=\"gpt-3.5-turbo\", messages=messages\n","        )\n","        #put the response gotten from the chatgpt function to the reply variable\n","        reply = chat.choices[0].message.content\n","        #append the response of the chatgpt to the message list to store the conversation\n","        messages.append({\"role\": \"assistant\", \"content\": reply})\n","        return reply\n","\n","#Interface of the chatbot built with gradio\n","\n","inputs = gr.Textbox(lines=10, label=\"Chat with Lizzy\")\n","outputs = gr.Textbox(label=\"Reply\")\n","\n","gr.Interface(fn=chatbot, inputs=inputs, outputs=outputs, title=\"Social Media Depression Chatbot Detector\",\n","             description=\"Be Comfortable and Chat with lizzy\",\n","             theme=\"compact\").launch(share=False,debug=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"29d425bb"},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"","provenance":[{"file_id":"1-z5jDURruxL6fgvgE7NBJMB3EtE_IOb7","timestamp":1695351471724}],"version":""},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":5}